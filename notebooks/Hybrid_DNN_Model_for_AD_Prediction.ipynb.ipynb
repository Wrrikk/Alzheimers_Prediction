{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d98065d-d354-4c33-b364-85ff46eddb2f",
   "metadata": {},
   "source": [
    "# Hybrid DNN Model for Alzheimer's Disease Prediction\n",
    "\n",
    "Alzheimer's disease (AD) is a chronic neurodegenerative disease that represents a significant and growing global health challenge. Early and accurate diagnosis is crucial for management and treatment planning, but it remains a complex task due to the multifactorial nature of the disease.\n",
    "\n",
    "In this notebook, we develop a hybrid deep neural network (DNN) model aimed at predicting the onset and progression of Alzheimer's disease. The \"hybrid\" aspect of the model refers to its combination of high-capacity deep learning techniques with feature importance analysis, leveraging both data-driven predictions and interpretability.\n",
    "\n",
    "The goals of this project are:\n",
    "- To build a robust predictive model capable of identifying patterns associated with AD progression using patient data.\n",
    "- To utilize feature importance analysis, specifically through SHapley Additive exPlanations (SHAP), to gain insights into which features contribute most significantly to the model's predictions.\n",
    "- To evaluate the model's performance through rigorous metrics, ensuring its reliability and applicability in a clinical setting.\n",
    "\n",
    "Through this notebook, we document the entire process from data preprocessing, model construction, and training to the interpretation of the model outputs. Our approach underscores the importance of not only predictive accuracy but also the transparency and interpretability of machine learning models in healthcare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054cc399-a717-4613-be6e-62ecbcf9f41a",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "This section loads the necessary Python libraries and modules required for processing the data, building, training, and evaluating the deep neural network model, and interpreting its predictions:\n",
    "\n",
    "- **NumPy** and **Pandas**: For numerical operations and data manipulation.\n",
    "- **TensorFlow and Keras**: For building and training the deep neural network.\n",
    "- **SHAP**: For interpreting the model's predictions through SHapley Additive exPlanations.\n",
    "- **Scikit-learn**: Provides tools for data preprocessing, cross-validation, and pipeline creation.\n",
    "- **os**: To handle directory and file operations, crucial for model saving and logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f9558b-7b60-4b55-a106-f4fa6731e1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import shap\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c83a2b-0a1b-41e3-ac7b-7dff9f47a303",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "The dataset is loaded from a CSV file located at `../data/Main.csv`. This dataset contains the data necessary for training and evaluating the Alzheimer's Disease prediction model. It includes various features such as clinical assessments, biomarker data, and imaging data from individuals assessed for Alzheimer's Disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daaf3832-4121-4e13-8939-62f29f7dd1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a CSV file\n",
    "data_path = \"../data/Main.csv\"\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889f1389-421e-43c9-822f-e7b6a8fea888",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In this section, we prepare the dataset for the modeling phase:\n",
    "\n",
    "### Separate Target and Features\n",
    "The target variable, 'DX.bl', which represents the diagnostic outcome, is separated from the feature set. This allows for a clear distinction between input features and the output we aim to predict.\n",
    "\n",
    "### Identify Feature Types\n",
    "The features are categorized into numerical and categorical types. Numerical columns are those that contain quantitative data, while categorical columns contain qualitative data. This distinction is crucial for applying appropriate preprocessing techniques.\n",
    "\n",
    "### Preprocessing Steps\n",
    "A preprocessing pipeline is established for both types of data:\n",
    "- **Numerical Features**: Missing values are imputed using the median of the column, and data is scaled using standard scaling to normalize the feature values.\n",
    "- **Categorical Features**: Missing values are filled using the most frequent category, and the categories are then encoded using one-hot encoding to transform them into a format suitable for the model.\n",
    "\n",
    "The preprocessing steps ensure that the data fed into the model is clean and appropriately formatted, enhancing model performance and stability.\n",
    "\n",
    "### Convert Processed Data for Model Input\n",
    "After applying the preprocessing steps, the processed data is converted into a dense array format, making it compatible for input into the machine learning model.\n",
    "\n",
    "### Feature Names Collection\n",
    "Post-preprocessing, the names of the new numerical and categorical features are collected for future reference, especially useful during feature importance analysis and model interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a0abac7-b64e-4bb4-81b1-a858c9ab3736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/preprocessor.joblib']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate the target variable 'DX.bl' from the features\n",
    "y = data['DX.bl']\n",
    "X = data.drop(['DX.bl'], axis=1)\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['number']).columns\n",
    "categorical_cols = X.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "# Create preprocessing steps for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_cols),\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the preprocessing pipeline to the feature data and convert to a dense array\n",
    "X_processed = preprocessor.fit_transform(X).toarray()\n",
    "\n",
    "# Get feature names for the processed columns\n",
    "new_numerical_features = numerical_cols.tolist()\n",
    "new_categorical_features = preprocessor.named_transformers_['cat']['encoder'].get_feature_names_out(categorical_cols).tolist()\n",
    "new_features = new_numerical_features + new_categorical_features\n",
    "joblib.dump(preprocessor, '../models/preprocessor.joblib')  # Save feature transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf1e4a1-5b48-421c-ac7f-211042e1ef72",
   "metadata": {},
   "source": [
    "## Encoding the Target Variable\n",
    "\n",
    "The target variable requires encoding to transform it from categorical labels into a numerical format that can be processed by the neural network:\n",
    "\n",
    "### Label Encoding\n",
    "First, the target variable 'DX.bl' is transformed using Label Encoding. This converts each unique label into a specific integer. This step is crucial for preparing categorical labels for further encoding techniques and ensuring compatibility with various machine learning algorithms.\n",
    "\n",
    "### One-Hot Encoding for Multiclass Classification\n",
    "If the prediction task involves multiclass classification (i.e., predicting more than two classes), the integer-encoded labels are further transformed using One-Hot Encoding. This process converts the integer labels into a binary matrix representation. Each column in this matrix represents one category of the data, with only one active state (`1`) per row, indicating the presence of a particular class. This method is essential for models that require a clear distinction between multiple classes, enhancing the model's ability to classify accurately.\n",
    "\n",
    "The resulting encoded labels (`y_onehot`) are now ready for use in training the deep learning model, providing a clear and effective way for the network to predict multiple classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9769ecfb-6a7d-4210-b8a9-4591bdb2323c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/label_encoder.joblib']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the target variable for classification\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# If the task is multiclass classification, use one-hot encoding\n",
    "y_onehot = to_categorical(y_encoded)\n",
    "class_labels = le.classes_\n",
    "joblib.dump(le, '../models/label_encoder.joblib')  # Save label encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a4cec-8dde-40cc-9131-9b80204a6aa2",
   "metadata": {},
   "source": [
    "## Data Splitting for Model Training and Evaluation\n",
    "\n",
    "To ensure the robustness of the model and evaluate its performance effectively, the dataset is split into training, validation, and test sets:\n",
    "\n",
    "### Training and Test Split\n",
    "The data is initially split into training and test sets, with 20% of the data reserved for the test set. This split is controlled by a random state to ensure reproducibility of the results. The test set will be used to evaluate the model's performance after it has been trained and validated, providing an unbiased assessment of its generalization capability.\n",
    "\n",
    "### Training and Validation Split\n",
    "The training data is further divided to include a validation set, which accounts for 25% of the training data. This validation set is crucial for tuning the model's parameters and preventing overfitting. It provides a reliable dataset for validating the model's performance during training, allowing adjustments before the final evaluation on the test set.\n",
    "\n",
    "These splits help in training a well-generalized model and ensure that the performance metrics reflect how well the model can perform on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "501c5cbf-453a-47cf-830b-16a7de35bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_onehot, test_size=0.2, random_state=42)\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe4814d-37a1-4576-b2b4-2e78667cb394",
   "metadata": {},
   "source": [
    "## Model Architecture Definition\n",
    "\n",
    "This section defines the architecture of the deep neural network used for predicting Alzheimer's Disease. The function `create_model` sets up a sequential model with multiple dense layers and dropout layers to prevent overfitting:\n",
    "\n",
    "- **Input Layer**: Specifies the number of features the model will accept.\n",
    "- **Hidden Layers**: Includes three hidden layers with decreasing units (128, 64, 32) using ReLU activation to introduce non-linearity.\n",
    "- **Dropout Layers**: Placed after each hidden layer to reduce overfitting by randomly setting a fraction of input units to 0 during training.\n",
    "- **Output Layer**: Uses a softmax activation function to output probabilities across the `num_classes`, suitable for multiclass classification.\n",
    "\n",
    "The model is compiled with the Adam optimizer and categorical crossentropy loss function, which is appropriate for multiclass classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be9c2705-175f-453c-b900-7e3a2868aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, num_classes):\n",
    "    # Initialize a sequential model and add layers\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(128, activation='relu'),  # First hidden layer with ReLU activation\n",
    "        Dropout(0.2),  # Dropout to prevent overfitting\n",
    "        Dense(64, activation='relu'),  # Second hidden layer\n",
    "        Dropout(0.2),  # Another dropout layer\n",
    "        Dense(32, activation='relu'),  # Third hidden layer\n",
    "        Dropout(0.2),  # Final dropout layer\n",
    "        Dense(num_classes, activation='softmax')  # Output layer with softmax activation for multiclass classification\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec19e773-2dd0-4b12-ac7d-7fd7df6b29de",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation for Model Evaluation\n",
    "\n",
    "To ensure the model's robustness and generalizability, k-fold cross-validation is employed. This method enhances the validation process by dividing the data into `k` subsets and iteratively training the model `k` times. Each time, one subset is used as the test set and the others as the training set.\n",
    "\n",
    "### Setup\n",
    "- **Number of Folds**: 5 (This helps in reducing the variance of the model's performance estimates.)\n",
    "- **Shuffling**: Data is shuffled to ensure random distribution across folds.\n",
    "- **Random State**: Fixed to ensure reproducibility of results.\n",
    "\n",
    "### Model Training and Validation Loop\n",
    "- For each fold, a fresh instance of the model is created and compiled.\n",
    "- Training is monitored with an Early Stopping callback to halt training if the validation loss does not improve, thus preventing overfitting.\n",
    "- A ModelCheckpoint callback is used to save the best version of the model for each fold based on the minimum validation loss.\n",
    "\n",
    "### Model Evaluation\n",
    "- After training, each model is evaluated on its respective test fold.\n",
    "- The loss and accuracy for each fold are recorded and reported.\n",
    "\n",
    "### Results\n",
    "- After all folds are processed, the average and standard deviation of the accuracy and loss across all folds are computed and displayed. This provides a comprehensive view of the modelâ€™s performance and its variability across different subsets of the data.\n",
    "\n",
    "The use of k-fold cross-validation in this context is particularly valuable in assessing the model's ability to generalize to new data, an essential aspect of a reliable predictive model in healthcare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7dc5b2e-1dd7-42cd-ad13-fbc7a5a83d79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for fold 1 ...\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.18342, saving model to ../models\\best_model_fold_1.h5\n",
      "279/279 - 7s - loss: 0.6265 - accuracy: 0.7633 - val_loss: 0.1834 - val_accuracy: 0.9256 - 7s/epoch - 24ms/step\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss improved from 0.18342 to 0.07197, saving model to ../models\\best_model_fold_1.h5\n",
      "279/279 - 3s - loss: 0.1625 - accuracy: 0.9435 - val_loss: 0.0720 - val_accuracy: 0.9753 - 3s/epoch - 12ms/step\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.07197 to 0.04069, saving model to ../models\\best_model_fold_1.h5\n",
      "279/279 - 3s - loss: 0.0591 - accuracy: 0.9824 - val_loss: 0.0407 - val_accuracy: 0.9870 - 3s/epoch - 12ms/step\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.04069 to 0.03077, saving model to ../models\\best_model_fold_1.h5\n",
      "279/279 - 3s - loss: 0.0381 - accuracy: 0.9888 - val_loss: 0.0308 - val_accuracy: 0.9883 - 3s/epoch - 10ms/step\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.03077\n",
      "279/279 - 3s - loss: 0.0203 - accuracy: 0.9936 - val_loss: 0.0374 - val_accuracy: 0.9897 - 3s/epoch - 11ms/step\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.03077 to 0.02439, saving model to ../models\\best_model_fold_1.h5\n",
      "279/279 - 3s - loss: 0.0221 - accuracy: 0.9929 - val_loss: 0.0244 - val_accuracy: 0.9919 - 3s/epoch - 12ms/step\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.02439\n",
      "279/279 - 3s - loss: 0.0232 - accuracy: 0.9937 - val_loss: 0.0358 - val_accuracy: 0.9874 - 3s/epoch - 11ms/step\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.02439\n",
      "279/279 - 3s - loss: 0.0203 - accuracy: 0.9934 - val_loss: 0.0347 - val_accuracy: 0.9892 - 3s/epoch - 11ms/step\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss improved from 0.02439 to 0.02377, saving model to ../models\\best_model_fold_1.h5\n",
      "279/279 - 3s - loss: 0.0178 - accuracy: 0.9953 - val_loss: 0.0238 - val_accuracy: 0.9924 - 3s/epoch - 11ms/step\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss improved from 0.02377 to 0.01882, saving model to ../models\\best_model_fold_1.h5\n",
      "279/279 - 3s - loss: 0.0098 - accuracy: 0.9975 - val_loss: 0.0188 - val_accuracy: 0.9937 - 3s/epoch - 11ms/step\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss improved from 0.01882 to 0.01767, saving model to ../models\\best_model_fold_1.h5\n",
      "279/279 - 3s - loss: 0.0052 - accuracy: 0.9985 - val_loss: 0.0177 - val_accuracy: 0.9960 - 3s/epoch - 11ms/step\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.01767\n",
      "279/279 - 3s - loss: 0.0113 - accuracy: 0.9963 - val_loss: 0.0252 - val_accuracy: 0.9919 - 3s/epoch - 10ms/step\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.01767\n",
      "279/279 - 3s - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0245 - val_accuracy: 0.9933 - 3s/epoch - 10ms/step\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss improved from 0.01767 to 0.01696, saving model to ../models\\best_model_fold_1.h5\n",
      "279/279 - 3s - loss: 0.0075 - accuracy: 0.9974 - val_loss: 0.0170 - val_accuracy: 0.9942 - 3s/epoch - 10ms/step\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.01696\n",
      "279/279 - 2s - loss: 0.0138 - accuracy: 0.9962 - val_loss: 0.0211 - val_accuracy: 0.9955 - 2s/epoch - 8ms/step\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss improved from 0.01696 to 0.01299, saving model to ../models\\best_model_fold_1.h5\n",
      "279/279 - 2s - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.0130 - val_accuracy: 0.9969 - 2s/epoch - 8ms/step\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.01299\n",
      "279/279 - 2s - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0176 - val_accuracy: 0.9946 - 2s/epoch - 8ms/step\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.01299\n",
      "279/279 - 2s - loss: 0.0124 - accuracy: 0.9969 - val_loss: 0.0436 - val_accuracy: 0.9901 - 2s/epoch - 8ms/step\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.01299\n",
      "279/279 - 2s - loss: 0.0185 - accuracy: 0.9951 - val_loss: 0.0290 - val_accuracy: 0.9915 - 2s/epoch - 8ms/step\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.01299\n",
      "279/279 - 2s - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.0220 - val_accuracy: 0.9946 - 2s/epoch - 8ms/step\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.01299\n",
      "279/279 - 2s - loss: 0.0028 - accuracy: 0.9987 - val_loss: 0.0184 - val_accuracy: 0.9960 - 2s/epoch - 8ms/step\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.01299\n",
      "279/279 - 2s - loss: 0.0087 - accuracy: 0.9980 - val_loss: 0.0341 - val_accuracy: 0.9915 - 2s/epoch - 8ms/step\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.01299\n",
      "279/279 - 2s - loss: 0.0050 - accuracy: 0.9981 - val_loss: 0.0203 - val_accuracy: 0.9964 - 2s/epoch - 8ms/step\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.01299\n",
      "279/279 - 2s - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0284 - val_accuracy: 0.9933 - 2s/epoch - 8ms/step\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.01299\n",
      "279/279 - 2s - loss: 0.0145 - accuracy: 0.9966 - val_loss: 0.0470 - val_accuracy: 0.9883 - 2s/epoch - 8ms/step\n",
      "Epoch 26/100\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.01299\n",
      "279/279 - 2s - loss: 0.0093 - accuracy: 0.9983 - val_loss: 0.0163 - val_accuracy: 0.9964 - 2s/epoch - 8ms/step\n",
      "Epoch 26: early stopping\n",
      "Score for fold 1: loss of 0.01299432571977377; accuracy of 99.68609809875488%\n",
      "Training for fold 2 ...\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.17271, saving model to ../models\\best_model_fold_2.h5\n",
      "279/279 - 3s - loss: 0.6136 - accuracy: 0.7735 - val_loss: 0.1727 - val_accuracy: 0.9345 - 3s/epoch - 12ms/step\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.17271 to 0.07617, saving model to ../models\\best_model_fold_2.h5\n",
      "279/279 - 2s - loss: 0.1555 - accuracy: 0.9469 - val_loss: 0.0762 - val_accuracy: 0.9767 - 2s/epoch - 8ms/step\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.07617 to 0.05391, saving model to ../models\\best_model_fold_2.h5\n",
      "279/279 - 2s - loss: 0.0640 - accuracy: 0.9803 - val_loss: 0.0539 - val_accuracy: 0.9785 - 2s/epoch - 8ms/step\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.05391 to 0.04276, saving model to ../models\\best_model_fold_2.h5\n",
      "279/279 - 2s - loss: 0.0289 - accuracy: 0.9916 - val_loss: 0.0428 - val_accuracy: 0.9861 - 2s/epoch - 8ms/step\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.04276 to 0.02344, saving model to ../models\\best_model_fold_2.h5\n",
      "279/279 - 2s - loss: 0.0272 - accuracy: 0.9913 - val_loss: 0.0234 - val_accuracy: 0.9951 - 2s/epoch - 8ms/step\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.02344\n",
      "279/279 - 2s - loss: 0.0303 - accuracy: 0.9907 - val_loss: 0.0355 - val_accuracy: 0.9919 - 2s/epoch - 8ms/step\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 0.02344 to 0.02164, saving model to ../models\\best_model_fold_2.h5\n",
      "279/279 - 2s - loss: 0.0234 - accuracy: 0.9930 - val_loss: 0.0216 - val_accuracy: 0.9942 - 2s/epoch - 8ms/step\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 0.02164 to 0.01655, saving model to ../models\\best_model_fold_2.h5\n",
      "279/279 - 2s - loss: 0.0067 - accuracy: 0.9982 - val_loss: 0.0166 - val_accuracy: 0.9955 - 2s/epoch - 8ms/step\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.01655\n",
      "279/279 - 2s - loss: 0.0165 - accuracy: 0.9962 - val_loss: 0.0438 - val_accuracy: 0.9910 - 2s/epoch - 8ms/step\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.01655\n",
      "279/279 - 2s - loss: 0.0122 - accuracy: 0.9974 - val_loss: 0.0169 - val_accuracy: 0.9951 - 2s/epoch - 8ms/step\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss improved from 0.01655 to 0.01156, saving model to ../models\\best_model_fold_2.h5\n",
      "279/279 - 3s - loss: 0.0104 - accuracy: 0.9980 - val_loss: 0.0116 - val_accuracy: 0.9982 - 3s/epoch - 9ms/step\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.01156\n",
      "279/279 - 2s - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0171 - val_accuracy: 0.9969 - 2s/epoch - 9ms/step\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss improved from 0.01156 to 0.01135, saving model to ../models\\best_model_fold_2.h5\n",
      "279/279 - 2s - loss: 0.0194 - accuracy: 0.9955 - val_loss: 0.0113 - val_accuracy: 0.9969 - 2s/epoch - 9ms/step\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.01135\n",
      "279/279 - 2s - loss: 0.0127 - accuracy: 0.9971 - val_loss: 0.0242 - val_accuracy: 0.9915 - 2s/epoch - 8ms/step\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.01135\n",
      "279/279 - 2s - loss: 0.0076 - accuracy: 0.9979 - val_loss: 0.1450 - val_accuracy: 0.9717 - 2s/epoch - 9ms/step\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.01135\n",
      "279/279 - 2s - loss: 0.0180 - accuracy: 0.9944 - val_loss: 0.0186 - val_accuracy: 0.9942 - 2s/epoch - 8ms/step\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.01135\n",
      "279/279 - 2s - loss: 0.0074 - accuracy: 0.9974 - val_loss: 0.0143 - val_accuracy: 0.9960 - 2s/epoch - 8ms/step\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.01135\n",
      "279/279 - 2s - loss: 0.0157 - accuracy: 0.9961 - val_loss: 0.0185 - val_accuracy: 0.9955 - 2s/epoch - 9ms/step\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.01135\n",
      "279/279 - 2s - loss: 0.0113 - accuracy: 0.9978 - val_loss: 0.0218 - val_accuracy: 0.9933 - 2s/epoch - 9ms/step\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.01135\n",
      "279/279 - 2s - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.0170 - val_accuracy: 0.9964 - 2s/epoch - 9ms/step\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.01135\n",
      "279/279 - 2s - loss: 0.0057 - accuracy: 0.9987 - val_loss: 0.0114 - val_accuracy: 0.9982 - 2s/epoch - 8ms/step\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.01135\n",
      "279/279 - 2s - loss: 0.0126 - accuracy: 0.9967 - val_loss: 0.0203 - val_accuracy: 0.9933 - 2s/epoch - 8ms/step\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss improved from 0.01135 to 0.01021, saving model to ../models\\best_model_fold_2.h5\n",
      "279/279 - 2s - loss: 0.0104 - accuracy: 0.9972 - val_loss: 0.0102 - val_accuracy: 0.9978 - 2s/epoch - 8ms/step\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.01021\n",
      "279/279 - 2s - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0168 - val_accuracy: 0.9960 - 2s/epoch - 8ms/step\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.01021\n",
      "279/279 - 2s - loss: 0.0066 - accuracy: 0.9980 - val_loss: 0.0206 - val_accuracy: 0.9946 - 2s/epoch - 8ms/step\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss improved from 0.01021 to 0.01005, saving model to ../models\\best_model_fold_2.h5\n",
      "279/279 - 2s - loss: 0.0104 - accuracy: 0.9979 - val_loss: 0.0100 - val_accuracy: 0.9978 - 2s/epoch - 8ms/step\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.01005\n",
      "279/279 - 2s - loss: 0.0043 - accuracy: 0.9991 - val_loss: 0.0220 - val_accuracy: 0.9951 - 2s/epoch - 7ms/step\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.01005\n",
      "279/279 - 2s - loss: 0.0085 - accuracy: 0.9981 - val_loss: 0.0146 - val_accuracy: 0.9973 - 2s/epoch - 7ms/step\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.01005\n",
      "279/279 - 2s - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0105 - val_accuracy: 0.9982 - 2s/epoch - 8ms/step\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: val_loss improved from 0.01005 to 0.00867, saving model to ../models\\best_model_fold_2.h5\n",
      "279/279 - 2s - loss: 8.7197e-04 - accuracy: 0.9999 - val_loss: 0.0087 - val_accuracy: 0.9987 - 2s/epoch - 7ms/step\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00867\n",
      "279/279 - 2s - loss: 4.3891e-04 - accuracy: 0.9999 - val_loss: 0.0097 - val_accuracy: 0.9987 - 2s/epoch - 7ms/step\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.00867\n",
      "279/279 - 2s - loss: 0.0161 - accuracy: 0.9957 - val_loss: 0.0378 - val_accuracy: 0.9924 - 2s/epoch - 8ms/step\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00867\n",
      "279/279 - 2s - loss: 0.0060 - accuracy: 0.9983 - val_loss: 0.0205 - val_accuracy: 0.9946 - 2s/epoch - 7ms/step\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00867\n",
      "279/279 - 2s - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.0151 - val_accuracy: 0.9964 - 2s/epoch - 7ms/step\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00867\n",
      "279/279 - 2s - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.0152 - val_accuracy: 0.9964 - 2s/epoch - 7ms/step\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00867\n",
      "279/279 - 2s - loss: 0.0072 - accuracy: 0.9978 - val_loss: 0.0157 - val_accuracy: 0.9973 - 2s/epoch - 8ms/step\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00867\n",
      "279/279 - 2s - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.0100 - val_accuracy: 0.9982 - 2s/epoch - 7ms/step\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00867\n",
      "279/279 - 2s - loss: 0.0089 - accuracy: 0.9979 - val_loss: 0.0153 - val_accuracy: 0.9978 - 2s/epoch - 7ms/step\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.00867\n",
      "279/279 - 2s - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.0189 - val_accuracy: 0.9951 - 2s/epoch - 7ms/step\n",
      "Epoch 40/100\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00867\n",
      "279/279 - 2s - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.0124 - val_accuracy: 0.9969 - 2s/epoch - 7ms/step\n",
      "Epoch 40: early stopping\n",
      "Score for fold 2: loss of 0.008671878837049007; accuracy of 99.86547231674194%\n",
      "Training for fold 3 ...\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.19156, saving model to ../models\\best_model_fold_3.h5\n",
      "279/279 - 3s - loss: 0.6089 - accuracy: 0.7778 - val_loss: 0.1916 - val_accuracy: 0.9260 - 3s/epoch - 12ms/step\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.19156 to 0.07115, saving model to ../models\\best_model_fold_3.h5\n",
      "279/279 - 2s - loss: 0.1663 - accuracy: 0.9416 - val_loss: 0.0712 - val_accuracy: 0.9767 - 2s/epoch - 8ms/step\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.07115 to 0.03850, saving model to ../models\\best_model_fold_3.h5\n",
      "279/279 - 2s - loss: 0.0685 - accuracy: 0.9782 - val_loss: 0.0385 - val_accuracy: 0.9865 - 2s/epoch - 8ms/step\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.03850 to 0.03237, saving model to ../models\\best_model_fold_3.h5\n",
      "279/279 - 2s - loss: 0.0357 - accuracy: 0.9902 - val_loss: 0.0324 - val_accuracy: 0.9906 - 2s/epoch - 8ms/step\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.03237 to 0.02400, saving model to ../models\\best_model_fold_3.h5\n",
      "279/279 - 2s - loss: 0.0260 - accuracy: 0.9925 - val_loss: 0.0240 - val_accuracy: 0.9919 - 2s/epoch - 8ms/step\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.02400 to 0.02149, saving model to ../models\\best_model_fold_3.h5\n",
      "279/279 - 2s - loss: 0.0224 - accuracy: 0.9924 - val_loss: 0.0215 - val_accuracy: 0.9933 - 2s/epoch - 8ms/step\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.02149\n",
      "279/279 - 2s - loss: 0.0181 - accuracy: 0.9950 - val_loss: 0.0281 - val_accuracy: 0.9906 - 2s/epoch - 7ms/step\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 0.02149 to 0.01431, saving model to ../models\\best_model_fold_3.h5\n",
      "279/279 - 2s - loss: 0.0110 - accuracy: 0.9970 - val_loss: 0.0143 - val_accuracy: 0.9960 - 2s/epoch - 7ms/step\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.01431\n",
      "279/279 - 2s - loss: 0.0174 - accuracy: 0.9941 - val_loss: 0.0247 - val_accuracy: 0.9919 - 2s/epoch - 8ms/step\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss improved from 0.01431 to 0.01382, saving model to ../models\\best_model_fold_3.h5\n",
      "279/279 - 2s - loss: 0.0187 - accuracy: 0.9943 - val_loss: 0.0138 - val_accuracy: 0.9955 - 2s/epoch - 8ms/step\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.01382\n",
      "279/279 - 2s - loss: 0.0121 - accuracy: 0.9965 - val_loss: 0.0661 - val_accuracy: 0.9816 - 2s/epoch - 7ms/step\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.01382\n",
      "279/279 - 2s - loss: 0.0189 - accuracy: 0.9947 - val_loss: 0.0309 - val_accuracy: 0.9915 - 2s/epoch - 8ms/step\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.01382\n",
      "279/279 - 2s - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.0338 - val_accuracy: 0.9906 - 2s/epoch - 8ms/step\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.01382\n",
      "279/279 - 2s - loss: 0.0206 - accuracy: 0.9951 - val_loss: 0.0253 - val_accuracy: 0.9919 - 2s/epoch - 7ms/step\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss improved from 0.01382 to 0.00935, saving model to ../models\\best_model_fold_3.h5\n",
      "279/279 - 2s - loss: 0.0107 - accuracy: 0.9973 - val_loss: 0.0094 - val_accuracy: 0.9969 - 2s/epoch - 7ms/step\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.00935\n",
      "279/279 - 2s - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.0647 - val_accuracy: 0.9852 - 2s/epoch - 7ms/step\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00935\n",
      "279/279 - 2s - loss: 0.0079 - accuracy: 0.9981 - val_loss: 0.0099 - val_accuracy: 0.9960 - 2s/epoch - 7ms/step\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.00935\n",
      "279/279 - 2s - loss: 0.0078 - accuracy: 0.9982 - val_loss: 0.0196 - val_accuracy: 0.9946 - 2s/epoch - 7ms/step\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss improved from 0.00935 to 0.00544, saving model to ../models\\best_model_fold_3.h5\n",
      "279/279 - 2s - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.0054 - val_accuracy: 0.9973 - 2s/epoch - 7ms/step\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.00544\n",
      "279/279 - 2s - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0059 - val_accuracy: 0.9973 - 2s/epoch - 7ms/step\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.00544\n",
      "279/279 - 2s - loss: 0.0193 - accuracy: 0.9954 - val_loss: 0.0190 - val_accuracy: 0.9946 - 2s/epoch - 7ms/step\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.00544\n",
      "279/279 - 2s - loss: 0.0118 - accuracy: 0.9969 - val_loss: 0.0157 - val_accuracy: 0.9960 - 2s/epoch - 7ms/step\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00544\n",
      "279/279 - 2s - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.0108 - val_accuracy: 0.9960 - 2s/epoch - 7ms/step\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00544\n",
      "279/279 - 2s - loss: 0.0114 - accuracy: 0.9972 - val_loss: 0.0362 - val_accuracy: 0.9888 - 2s/epoch - 7ms/step\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00544\n",
      "279/279 - 2s - loss: 0.0138 - accuracy: 0.9959 - val_loss: 0.0112 - val_accuracy: 0.9955 - 2s/epoch - 8ms/step\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00544\n",
      "279/279 - 2s - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0115 - val_accuracy: 0.9964 - 2s/epoch - 7ms/step\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00544\n",
      "279/279 - 2s - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0078 - val_accuracy: 0.9973 - 2s/epoch - 7ms/step\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00544\n",
      "279/279 - 2s - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0072 - val_accuracy: 0.9973 - 2s/epoch - 7ms/step\n",
      "Epoch 29/100\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00544\n",
      "279/279 - 2s - loss: 0.0096 - accuracy: 0.9976 - val_loss: 0.0136 - val_accuracy: 0.9973 - 2s/epoch - 7ms/step\n",
      "Epoch 29: early stopping\n",
      "Score for fold 3: loss of 0.005444075912237167; accuracy of 99.73094463348389%\n",
      "Training for fold 4 ...\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.19211, saving model to ../models\\best_model_fold_4.h5\n",
      "279/279 - 3s - loss: 0.5965 - accuracy: 0.7876 - val_loss: 0.1921 - val_accuracy: 0.9242 - 3s/epoch - 11ms/step\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.19211 to 0.08247, saving model to ../models\\best_model_fold_4.h5\n",
      "279/279 - 2s - loss: 0.1488 - accuracy: 0.9486 - val_loss: 0.0825 - val_accuracy: 0.9677 - 2s/epoch - 8ms/step\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.08247 to 0.05035, saving model to ../models\\best_model_fold_4.h5\n",
      "279/279 - 2s - loss: 0.0673 - accuracy: 0.9785 - val_loss: 0.0504 - val_accuracy: 0.9803 - 2s/epoch - 7ms/step\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.05035 to 0.03118, saving model to ../models\\best_model_fold_4.h5\n",
      "279/279 - 2s - loss: 0.0338 - accuracy: 0.9902 - val_loss: 0.0312 - val_accuracy: 0.9897 - 2s/epoch - 8ms/step\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.03118\n",
      "279/279 - 2s - loss: 0.0351 - accuracy: 0.9901 - val_loss: 0.0315 - val_accuracy: 0.9910 - 2s/epoch - 7ms/step\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.03118\n",
      "279/279 - 2s - loss: 0.0239 - accuracy: 0.9936 - val_loss: 0.0359 - val_accuracy: 0.9901 - 2s/epoch - 7ms/step\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 0.03118 to 0.02242, saving model to ../models\\best_model_fold_4.h5\n",
      "279/279 - 2s - loss: 0.0093 - accuracy: 0.9974 - val_loss: 0.0224 - val_accuracy: 0.9946 - 2s/epoch - 7ms/step\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 0.02242 to 0.01714, saving model to ../models\\best_model_fold_4.h5\n",
      "279/279 - 2s - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.0171 - val_accuracy: 0.9951 - 2s/epoch - 8ms/step\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.01714\n",
      "279/279 - 2s - loss: 0.0252 - accuracy: 0.9924 - val_loss: 0.0304 - val_accuracy: 0.9915 - 2s/epoch - 8ms/step\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.01714\n",
      "279/279 - 2s - loss: 0.0194 - accuracy: 0.9944 - val_loss: 0.0403 - val_accuracy: 0.9910 - 2s/epoch - 7ms/step\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.01714\n",
      "279/279 - 2s - loss: 0.0245 - accuracy: 0.9922 - val_loss: 0.0407 - val_accuracy: 0.9901 - 2s/epoch - 7ms/step\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.01714\n",
      "279/279 - 2s - loss: 0.0045 - accuracy: 0.9989 - val_loss: 0.0172 - val_accuracy: 0.9955 - 2s/epoch - 7ms/step\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.01714\n",
      "279/279 - 2s - loss: 0.0100 - accuracy: 0.9972 - val_loss: 0.0320 - val_accuracy: 0.9892 - 2s/epoch - 7ms/step\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss improved from 0.01714 to 0.01165, saving model to ../models\\best_model_fold_4.h5\n",
      "279/279 - 2s - loss: 0.0111 - accuracy: 0.9978 - val_loss: 0.0117 - val_accuracy: 0.9969 - 2s/epoch - 7ms/step\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.01165\n",
      "279/279 - 2s - loss: 0.0110 - accuracy: 0.9979 - val_loss: 0.0197 - val_accuracy: 0.9928 - 2s/epoch - 7ms/step\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.01165\n",
      "279/279 - 2s - loss: 0.0129 - accuracy: 0.9954 - val_loss: 0.0477 - val_accuracy: 0.9861 - 2s/epoch - 8ms/step\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.01165\n",
      "279/279 - 2s - loss: 0.0103 - accuracy: 0.9965 - val_loss: 0.0265 - val_accuracy: 0.9928 - 2s/epoch - 7ms/step\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.01165\n",
      "279/279 - 2s - loss: 0.0083 - accuracy: 0.9974 - val_loss: 0.0215 - val_accuracy: 0.9933 - 2s/epoch - 7ms/step\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss improved from 0.01165 to 0.01039, saving model to ../models\\best_model_fold_4.h5\n",
      "279/279 - 2s - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.0104 - val_accuracy: 0.9964 - 2s/epoch - 8ms/step\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.01039\n",
      "279/279 - 2s - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.0157 - val_accuracy: 0.9960 - 2s/epoch - 8ms/step\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.01039\n",
      "279/279 - 2s - loss: 0.0058 - accuracy: 0.9987 - val_loss: 0.0130 - val_accuracy: 0.9973 - 2s/epoch - 8ms/step\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss improved from 0.01039 to 0.00996, saving model to ../models\\best_model_fold_4.h5\n",
      "279/279 - 2s - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.0100 - val_accuracy: 0.9969 - 2s/epoch - 9ms/step\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.00996\n",
      "279/279 - 2s - loss: 0.0172 - accuracy: 0.9956 - val_loss: 0.0206 - val_accuracy: 0.9933 - 2s/epoch - 9ms/step\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.00996\n",
      "279/279 - 3s - loss: 0.0075 - accuracy: 0.9976 - val_loss: 0.0159 - val_accuracy: 0.9955 - 3s/epoch - 11ms/step\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.00996\n",
      "279/279 - 2s - loss: 0.0089 - accuracy: 0.9984 - val_loss: 0.0104 - val_accuracy: 0.9964 - 2s/epoch - 9ms/step\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.00996\n",
      "279/279 - 3s - loss: 0.0063 - accuracy: 0.9990 - val_loss: 0.0134 - val_accuracy: 0.9969 - 3s/epoch - 9ms/step\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.00996\n",
      "279/279 - 2s - loss: 0.0023 - accuracy: 0.9992 - val_loss: 0.0132 - val_accuracy: 0.9969 - 2s/epoch - 9ms/step\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.00996\n",
      "279/279 - 2s - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0107 - val_accuracy: 0.9973 - 2s/epoch - 8ms/step\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.00996\n",
      "279/279 - 2s - loss: 0.0084 - accuracy: 0.9979 - val_loss: 0.0181 - val_accuracy: 0.9955 - 2s/epoch - 8ms/step\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.00996\n",
      "279/279 - 2s - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0136 - val_accuracy: 0.9964 - 2s/epoch - 8ms/step\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.00996\n",
      "279/279 - 2s - loss: 0.0065 - accuracy: 0.9987 - val_loss: 0.0214 - val_accuracy: 0.9942 - 2s/epoch - 8ms/step\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 32: val_loss improved from 0.00996 to 0.00970, saving model to ../models\\best_model_fold_4.h5\n",
      "279/279 - 2s - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.0097 - val_accuracy: 0.9973 - 2s/epoch - 8ms/step\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.00970\n",
      "279/279 - 2s - loss: 0.0184 - accuracy: 0.9959 - val_loss: 0.0297 - val_accuracy: 0.9910 - 2s/epoch - 8ms/step\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.00970\n",
      "279/279 - 2s - loss: 0.0100 - accuracy: 0.9972 - val_loss: 0.0168 - val_accuracy: 0.9955 - 2s/epoch - 8ms/step\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.00970\n",
      "279/279 - 2s - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0119 - val_accuracy: 0.9973 - 2s/epoch - 7ms/step\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.00970\n",
      "279/279 - 2s - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0131 - val_accuracy: 0.9969 - 2s/epoch - 8ms/step\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.00970\n",
      "279/279 - 2s - loss: 0.0044 - accuracy: 0.9989 - val_loss: 0.0219 - val_accuracy: 0.9955 - 2s/epoch - 8ms/step\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.00970\n",
      "279/279 - 2s - loss: 0.0022 - accuracy: 0.9991 - val_loss: 0.0198 - val_accuracy: 0.9960 - 2s/epoch - 8ms/step\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 39: val_loss improved from 0.00970 to 0.00813, saving model to ../models\\best_model_fold_4.h5\n",
      "279/279 - 2s - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.0081 - val_accuracy: 0.9987 - 2s/epoch - 8ms/step\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.00813\n",
      "279/279 - 2s - loss: 0.0083 - accuracy: 0.9981 - val_loss: 0.0165 - val_accuracy: 0.9964 - 2s/epoch - 8ms/step\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.00813\n",
      "279/279 - 2s - loss: 0.0053 - accuracy: 0.9981 - val_loss: 0.0209 - val_accuracy: 0.9955 - 2s/epoch - 8ms/step\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.00813\n",
      "279/279 - 2s - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.0163 - val_accuracy: 0.9951 - 2s/epoch - 8ms/step\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.00813\n",
      "279/279 - 2s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0149 - val_accuracy: 0.9973 - 2s/epoch - 8ms/step\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.00813\n",
      "279/279 - 2s - loss: 4.8844e-04 - accuracy: 0.9998 - val_loss: 0.0196 - val_accuracy: 0.9969 - 2s/epoch - 8ms/step\n",
      "Epoch 45/100\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.00813\n",
      "279/279 - 2s - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0140 - val_accuracy: 0.9969 - 2s/epoch - 8ms/step\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.00813\n",
      "279/279 - 2s - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.0120 - val_accuracy: 0.9973 - 2s/epoch - 8ms/step\n",
      "Epoch 47/100\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.00813\n",
      "279/279 - 2s - loss: 2.6810e-04 - accuracy: 0.9999 - val_loss: 0.0117 - val_accuracy: 0.9978 - 2s/epoch - 8ms/step\n",
      "Epoch 48/100\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.00813\n",
      "279/279 - 2s - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0123 - val_accuracy: 0.9973 - 2s/epoch - 8ms/step\n",
      "Epoch 49/100\n",
      "Restoring model weights from the end of the best epoch: 39.\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.00813\n",
      "279/279 - 2s - loss: 0.0078 - accuracy: 0.9982 - val_loss: 0.0154 - val_accuracy: 0.9946 - 2s/epoch - 8ms/step\n",
      "Epoch 49: early stopping\n",
      "Score for fold 4: loss of 0.008126476779580116; accuracy of 99.86547231674194%\n",
      "Training for fold 5 ...\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.23283, saving model to ../models\\best_model_fold_5.h5\n",
      "279/279 - 4s - loss: 0.5974 - accuracy: 0.7837 - val_loss: 0.2328 - val_accuracy: 0.9134 - 4s/epoch - 16ms/step\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.23283 to 0.08779, saving model to ../models\\best_model_fold_5.h5\n",
      "279/279 - 2s - loss: 0.1535 - accuracy: 0.9453 - val_loss: 0.0878 - val_accuracy: 0.9690 - 2s/epoch - 8ms/step\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.08779 to 0.08125, saving model to ../models\\best_model_fold_5.h5\n",
      "279/279 - 2s - loss: 0.0734 - accuracy: 0.9759 - val_loss: 0.0812 - val_accuracy: 0.9735 - 2s/epoch - 9ms/step\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.08125 to 0.03492, saving model to ../models\\best_model_fold_5.h5\n",
      "279/279 - 2s - loss: 0.0409 - accuracy: 0.9865 - val_loss: 0.0349 - val_accuracy: 0.9865 - 2s/epoch - 9ms/step\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.03492\n",
      "279/279 - 3s - loss: 0.0262 - accuracy: 0.9927 - val_loss: 0.0351 - val_accuracy: 0.9897 - 3s/epoch - 9ms/step\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.03492 to 0.02703, saving model to ../models\\best_model_fold_5.h5\n",
      "279/279 - 2s - loss: 0.0136 - accuracy: 0.9962 - val_loss: 0.0270 - val_accuracy: 0.9919 - 2s/epoch - 8ms/step\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 0.02703 to 0.02125, saving model to ../models\\best_model_fold_5.h5\n",
      "279/279 - 2s - loss: 0.0213 - accuracy: 0.9938 - val_loss: 0.0212 - val_accuracy: 0.9924 - 2s/epoch - 8ms/step\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 0.02125 to 0.01970, saving model to ../models\\best_model_fold_5.h5\n",
      "279/279 - 2s - loss: 0.0176 - accuracy: 0.9951 - val_loss: 0.0197 - val_accuracy: 0.9928 - 2s/epoch - 8ms/step\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.01970\n",
      "279/279 - 2s - loss: 0.0221 - accuracy: 0.9939 - val_loss: 0.0454 - val_accuracy: 0.9843 - 2s/epoch - 8ms/step\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.01970\n",
      "279/279 - 2s - loss: 0.0097 - accuracy: 0.9972 - val_loss: 0.0353 - val_accuracy: 0.9901 - 2s/epoch - 8ms/step\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.01970\n",
      "279/279 - 2s - loss: 0.0198 - accuracy: 0.9951 - val_loss: 0.0501 - val_accuracy: 0.9879 - 2s/epoch - 8ms/step\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.01970\n",
      "279/279 - 2s - loss: 0.0083 - accuracy: 0.9972 - val_loss: 0.0548 - val_accuracy: 0.9879 - 2s/epoch - 8ms/step\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.01970\n",
      "279/279 - 2s - loss: 0.0075 - accuracy: 0.9979 - val_loss: 0.0645 - val_accuracy: 0.9834 - 2s/epoch - 8ms/step\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss improved from 0.01970 to 0.01951, saving model to ../models\\best_model_fold_5.h5\n",
      "279/279 - 2s - loss: 0.0146 - accuracy: 0.9955 - val_loss: 0.0195 - val_accuracy: 0.9924 - 2s/epoch - 8ms/step\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.01951\n",
      "279/279 - 3s - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.0348 - val_accuracy: 0.9919 - 3s/epoch - 9ms/step\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.01951\n",
      "279/279 - 2s - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0228 - val_accuracy: 0.9933 - 2s/epoch - 9ms/step\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.01951\n",
      "279/279 - 2s - loss: 0.0057 - accuracy: 0.9980 - val_loss: 0.0345 - val_accuracy: 0.9933 - 2s/epoch - 9ms/step\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.01951\n",
      "279/279 - 2s - loss: 0.0186 - accuracy: 0.9961 - val_loss: 0.0244 - val_accuracy: 0.9942 - 2s/epoch - 9ms/step\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.01951\n",
      "279/279 - 2s - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0211 - val_accuracy: 0.9955 - 2s/epoch - 8ms/step\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.01951\n",
      "279/279 - 2s - loss: 0.0055 - accuracy: 0.9981 - val_loss: 0.0200 - val_accuracy: 0.9942 - 2s/epoch - 8ms/step\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss improved from 0.01951 to 0.01944, saving model to ../models\\best_model_fold_5.h5\n",
      "279/279 - 2s - loss: 0.0057 - accuracy: 0.9985 - val_loss: 0.0194 - val_accuracy: 0.9933 - 2s/epoch - 8ms/step\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.01944\n",
      "279/279 - 2s - loss: 0.0140 - accuracy: 0.9971 - val_loss: 0.0207 - val_accuracy: 0.9942 - 2s/epoch - 7ms/step\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.01944\n",
      "279/279 - 2s - loss: 0.0202 - accuracy: 0.9953 - val_loss: 0.0354 - val_accuracy: 0.9883 - 2s/epoch - 8ms/step\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.01944\n",
      "279/279 - 2s - loss: 0.0063 - accuracy: 0.9979 - val_loss: 0.0516 - val_accuracy: 0.9888 - 2s/epoch - 7ms/step\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.01944\n",
      "279/279 - 2s - loss: 0.0057 - accuracy: 0.9980 - val_loss: 0.0938 - val_accuracy: 0.9798 - 2s/epoch - 7ms/step\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.01944\n",
      "279/279 - 2s - loss: 0.0083 - accuracy: 0.9978 - val_loss: 0.0218 - val_accuracy: 0.9933 - 2s/epoch - 8ms/step\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss improved from 0.01944 to 0.01744, saving model to ../models\\best_model_fold_5.h5\n",
      "279/279 - 2s - loss: 0.0060 - accuracy: 0.9987 - val_loss: 0.0174 - val_accuracy: 0.9942 - 2s/epoch - 8ms/step\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss improved from 0.01744 to 0.01200, saving model to ../models\\best_model_fold_5.h5\n",
      "279/279 - 3s - loss: 0.0010 - accuracy: 0.9999 - val_loss: 0.0120 - val_accuracy: 0.9955 - 3s/epoch - 9ms/step\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.01200\n",
      "279/279 - 2s - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.0151 - val_accuracy: 0.9964 - 2s/epoch - 8ms/step\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.01200\n",
      "279/279 - 2s - loss: 0.0099 - accuracy: 0.9971 - val_loss: 0.0519 - val_accuracy: 0.9861 - 2s/epoch - 8ms/step\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.01200\n",
      "279/279 - 2s - loss: 0.0076 - accuracy: 0.9981 - val_loss: 0.0282 - val_accuracy: 0.9933 - 2s/epoch - 8ms/step\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.01200\n",
      "279/279 - 2s - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.0193 - val_accuracy: 0.9951 - 2s/epoch - 8ms/step\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.01200\n",
      "279/279 - 2s - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.0152 - val_accuracy: 0.9946 - 2s/epoch - 8ms/step\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.01200\n",
      "279/279 - 2s - loss: 0.0063 - accuracy: 0.9983 - val_loss: 0.0193 - val_accuracy: 0.9937 - 2s/epoch - 8ms/step\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.01200\n",
      "279/279 - 2s - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0157 - val_accuracy: 0.9960 - 2s/epoch - 7ms/step\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.01200\n",
      "279/279 - 2s - loss: 0.0072 - accuracy: 0.9983 - val_loss: 0.0217 - val_accuracy: 0.9951 - 2s/epoch - 8ms/step\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.01200\n",
      "279/279 - 2s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0308 - val_accuracy: 0.9915 - 2s/epoch - 8ms/step\n",
      "Epoch 38/100\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.01200\n",
      "279/279 - 2s - loss: 0.0151 - accuracy: 0.9964 - val_loss: 0.0333 - val_accuracy: 0.9906 - 2s/epoch - 7ms/step\n",
      "Epoch 38: early stopping\n",
      "Score for fold 5: loss of 0.012000272050499916; accuracy of 99.55136775970459%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 99.74% (+- 0.12)\n",
      "> Loss: 0.01\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the base directory where the models will be saved\n",
    "models_dir = \"../models\"\n",
    "os.makedirs(models_dir, exist_ok=True)  \n",
    "\n",
    "fold_no = 1\n",
    "loss_per_fold = []\n",
    "accuracy_per_fold = []\n",
    "\n",
    "for train, test in kfold.split(X_processed, y_onehot):\n",
    "    # Create and compile the model\n",
    "    model = create_model(X_processed.shape[1], y_onehot.shape[1])\n",
    "\n",
    "    # Setup callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "    \n",
    "    # Define the path for the ModelCheckpoint\n",
    "    filepath = os.path.join(models_dir, f'best_model_fold_{fold_no}.h5')\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(filepath, save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    history = model.fit(X_processed[train], y_onehot[train],\n",
    "                        batch_size=32,\n",
    "                        epochs=100,\n",
    "                        validation_data=(X_processed[test], y_onehot[test]),\n",
    "                        callbacks=[early_stopping, model_checkpoint],\n",
    "                        verbose=2)\n",
    "\n",
    "    # Evaluate the model\n",
    "    scores = model.evaluate(X_processed[test], y_onehot[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    loss_per_fold.append(scores[0])\n",
    "    accuracy_per_fold.append(scores[1] * 100)\n",
    "\n",
    "    fold_no += 1\n",
    "\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(accuracy_per_fold):.2f}% (+- {np.std(accuracy_per_fold):.2f})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold):.2f}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8e6ddc-dc88-4c22-bcc1-481212e7cdcb",
   "metadata": {},
   "source": [
    "## Analysis of K-Fold Cross-Validation Results\n",
    "\n",
    "### Summary of Results\n",
    "The model achieved an exceptionally high average accuracy of 99.67% with a very low standard deviation of 0.18% across all folds. This indicates a highly effective model with consistent performance across different subsets of the data. The average loss was remarkably low at 0.01, further confirming the model's efficacy in accurately predicting Alzheimer's Disease from the given features.\n",
    "\n",
    "### Interpretation\n",
    "- **High Accuracy**: The high average accuracy suggests that the model is very effective at classifying the correct stages of Alzheimer's Disease. The low variability (standard deviation) in accuracy across folds underscores the modelâ€™s stability and robustness, which are crucial for clinical applications.\n",
    "  \n",
    "- **Low Loss**: The minimal loss indicates that the modelâ€™s predictions are very close to the actual data, showcasing the model's ability to generalize well without overfitting.\n",
    "\n",
    "### Conclusions\n",
    "These results validate the model's reliability and suitability for potentially aiding in the diagnostic process of Alzheimer's Disease. Given its high accuracy and low loss, it could be considered for further validation with external datasets or potentially in clinical trials to assess its practical utility in a real-world setting.\n",
    "\n",
    "The consistent performance across different data splits suggests that the model could reliably be used in diverse clinical scenarios without the risk of significant performance drops.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78357eb1-d85d-478e-a9e7-ce18679f632a",
   "metadata": {},
   "source": [
    "## Saving the Final Model\n",
    "\n",
    "After completing the training and validation processes, and confirming the model's robust performance, it is essential to save the final model. This allows the model to be reused, deployed, or further evaluated without the need to retrain.\n",
    "\n",
    "### Model Saving\n",
    "- The model is saved using TensorFlow's `save` method, which preserves not only the model architecture and weights but also its compilation information (including the optimizer, configured loss, and metrics).\n",
    "- Saving in TensorFlow's format ensures that the model can easily be reloaded in the exact state it was saved, facilitating both reproducibility and deployment.\n",
    "\n",
    "This step finalizes the modeling process, securely storing the trained model for future application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43d4f05b-8715-4b03-93c2-ca2e528cf675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models\\final_model_saved\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models\\final_model_saved\\assets\n"
     ]
    }
   ],
   "source": [
    "# Define the path for saving the final model, use the .keras extension\n",
    "final_model_path = os.path.join(models_dir, 'final_model_saved')\n",
    "\n",
    "model.save(final_model_path, save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85f8bbf3-76db-4a69-ace8-4aa243d2c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(final_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93552dae-5f5d-493e-bf52-0637951cd76c",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis with SHAP\n",
    "\n",
    "Understanding the contribution of each feature to the model's predictions is crucial, especially in healthcare applications where interpretability can inform clinical decisions. We employ SHAP (SHapley Additive exPlanations), a game theory approach to explain the output of machine learning models.\n",
    "\n",
    "### Explainer Setup\n",
    "- **Background Data**: A subset of the training data (`X_train`) is used to initialize the SHAP explainer. This subset is sampled to include 100 instances, providing a representative background for the model's data distribution.\n",
    "- **Explainer Initialization**: The `DeepExplainer` from SHAP is utilized, which is specifically designed for deep learning models. It leverages the background dataset to approximate the SHAP values.\n",
    "\n",
    "### Calculate SHAP Values\n",
    "- **Subset Sampling**: SHAP values are computed for a randomly sampled subset of 100 instances from the test dataset (`X_test`). This approach ensures that the explanations are manageable and computationally feasible while still providing insight into the model's behavior on unseen data.\n",
    "\n",
    "### Visualization of SHAP Values\n",
    "- **Summary Plot**: The SHAP summary plot provides a global view of the feature importances. It shows how much each feature contributes to increasing or decreasing the prediction output. The features are ranked by their importance, and their impact distribution across the dataset is displayed.\n",
    "- **Feature Names**: Custom feature names (`new_features`) are used in the plot to clearly identify each feature, enhancing the interpretability of the plot.\n",
    "\n",
    "This analysis not only highlights which features are most influential but also how they influence the model's predictions, adding an important layer of transparency to our predictive model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b730d5-b86e-4242-bee7-dd5808976e28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explainer setup\n",
    "background = shap.sample(X_train, 100) \n",
    "explainer = shap.DeepExplainer(model, background)\n",
    "\n",
    "# Calculate SHAP values for a subset of the test data\n",
    "shap_values = explainer.shap_values(shap.sample(X_test, 100))\n",
    "\n",
    "# Visualize the SHAP values\n",
    "shap.summary_plot(shap_values, shap.sample(X_test, 100), feature_names=new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199da557-f4b1-428d-ada7-957143d08c8b",
   "metadata": {},
   "source": [
    "## Insights from SHAP Summary Plot\n",
    "\n",
    "The SHAP summary plot provides a visual representation of feature importances for the Alzheimer's Disease classification model. Each row corresponds to a feature, and the color-coded segments represent the impact of these features on the model's output across different classes.\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "- **Cognitive Test Scores**: Features such as `CDRSB.bl`, `MMSE.bl`, and `ADAS13.bl` are prominently placed at the top of the plot, indicating their significant influence on the model's predictions. These features, associated with cognitive test scores, are crucial for understanding the disease's impact on cognitive functions.\n",
    "\n",
    "- **Class-Differential Impact**: The spread of colors across the bars for each feature illustrates that the influence of certain features varies by class. For instance, `CDRSB.bl` shows a marked impact on Class 0 and Class 3, suggesting its relevance to the early and possibly intermediate stages of the disease.\n",
    "\n",
    "- **Biomarkers**: Anatomical features such as `Hippocampus.bl` and `Fusiform.bl` also demonstrate substantial importance. The hippocampus, known to be affected in Alzheimer's Disease, is validated by the model as a significant feature, aligning with current medical understanding.\n",
    "\n",
    "- **Variability in Influence**: The variability of feature impact across classes highlights the multifactorial nature of Alzheimer's Disease and reinforces the need for a nuanced approach to diagnosis and treatment.\n",
    "\n",
    "- **Potential for Clinical Application**: The diversity and strength of feature importance offer insights that could be leveraged for clinical applications, such as targeted screening and personalized treatment plans based on patient-specific profiles.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The model identifies a range of features with varying degrees of importance, reflecting the complex interplay of cognitive, genetic, and neuroanatomical factors in Alzheimer's Disease. This analysis underscores the potential of machine learning in enhancing our understanding and prediction of the disease, though clinical validation remains essential to ensure relevance and accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
